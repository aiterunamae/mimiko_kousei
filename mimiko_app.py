import streamlit as st
import json
import toml
import os
from pathlib import Path
from datetime import datetime
import pandas as pd
import io

# Google GenAI SDKã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import google.genai as genai
    from google.genai import types
    NEW_SDK = True
except ImportError:
    try:
        import google.generativeai as genai
        NEW_SDK = False
        st.warning("å¤ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚pip install google-genai ã§æ–°ã—ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
    except ImportError:
        st.error("Google GenAI ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install google-genai ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        st.stop()

# Vertex AIç”¨ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from google.auth import default
    from google.auth.transport.requests import Request
    from google.oauth2 import service_account
    VERTEX_AI_AVAILABLE = True
except ImportError:
    VERTEX_AI_AVAILABLE = False

# Load secrets
# Streamlit Cloudã®å ´åˆ
if hasattr(st, "secrets"):
    try:
        # æ–°ã—ã„å½¢å¼ã«å¯¾å¿œ
        vertex_ai_project_id = st.secrets["api"]["vertex_project"] if "api" in st.secrets and "vertex_project" in st.secrets["api"] else ""
        vertex_ai_location = st.secrets["api"]["vertex_location"] if "api" in st.secrets and "vertex_location" in st.secrets["api"] else "us-central1"
        default_model = st.secrets.get("default_model", "gemini-2.0-flash")
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæƒ…å ±
        gcp_service_account = dict(st.secrets["gcp_service_account"]) if "gcp_service_account" in st.secrets else None
    except Exception as e:
        st.error(f"Secretsã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        vertex_ai_project_id = ""
        vertex_ai_location = "us-central1"
        default_model = "gemini-2.0-flash"
        gcp_service_account = None
else:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ
    secrets_path = Path(__file__).parent / "secrets.toml"
    if secrets_path.exists():
        secrets = toml.load(secrets_path)
        api_config = secrets.get("api", {})
        vertex_ai_project_id = api_config.get("vertex_project", "")
        vertex_ai_location = api_config.get("vertex_location", "us-central1")
        default_model = secrets.get("default_model", "gemini-2.0-flash")
        gcp_service_account = secrets.get("gcp_service_account", None)
    else:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        vertex_ai_project_id = os.environ.get("VERTEX_AI_PROJECT_ID", "")
        vertex_ai_location = os.environ.get("VERTEX_AI_LOCATION", "us-central1")
        default_model = os.environ.get("DEFAULT_MODEL", "gemini-2.0-flash")
        gcp_service_account = None

# Vertex AI ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³
vertex_model_options = [
    "gemini-2.0-flash",
    "gemini-2.5-flash",
    "gemini-2.5-pro"
]

# Load correction prompts
def load_prompt(filename):
    prompt_path = Path(__file__).parent / filename
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()

# Load keyword CSV files
def load_keyword_csv(category):
    """ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰CSVã‚’èª­ã¿è¾¼ã‚€"""
    keyword_files = {
        "ãƒã‚¦ã‚¹": "ãƒã‚¦ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv",
        "ã‚µã‚¤ãƒ³": "ã‚µã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv",
        "å¤©ä½“": "å¤©ä½“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv",
        "ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆ": "ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv",
        "MPè»¸": "MPè»¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv",
        "ã‚¿ãƒ­ãƒƒãƒˆ": "ã‚¿ãƒ­ãƒƒãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv"
    }
    
    if category not in keyword_files:
        return None
    
    try:
        csv_path = Path(__file__).parent / keyword_files[category]
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        return df
    except Exception as e:
        st.warning(f"{category}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

# Get keyword details from CSV
def normalize_numbers(text):
    """å…¨è§’æ•°å­—ã‚’åŠè§’æ•°å­—ã«å¤‰æ›"""
    if not isinstance(text, str):
        return text
    # å…¨è§’æ•°å­—ã‚’åŠè§’æ•°å­—ã«å¤‰æ›
    trans_table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    return text.translate(trans_table)

def get_keyword_details(keywords_list):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—"""
    keyword_details = []
    
    for keyword_str in keywords_list:
        # "ã‚«ãƒ†ã‚´ãƒª: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" ã®å½¢å¼ã‹ã‚‰åˆ†è§£
        if ": " in keyword_str:
            category, keyword = keyword_str.split(": ", 1)
            
            # ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸCSVã‚’èª­ã¿è¾¼ã¿
            df = load_keyword_csv(category)
            if df is not None:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ä¸€è‡´ã™ã‚‹è¡Œã‚’æ¤œç´¢
                # æœ€åˆã®åˆ—ï¼ˆåç§°åˆ—ï¼‰ã§æ¤œç´¢
                name_col = df.columns[0]
                
                # æ•°å­—ã‚’æ­£è¦åŒ–ã—ã¦æ¤œç´¢
                normalized_keyword = normalize_numbers(keyword)
                
                # CSVå´ã®æ•°å­—ã‚‚æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
                df_normalized = df.copy()
                df_normalized[name_col] = df_normalized[name_col].apply(normalize_numbers)
                
                # å®Œå…¨ä¸€è‡´ã§æ¤œç´¢
                matching_rows = df_normalized[df_normalized[name_col] == normalized_keyword]
                
                # å®Œå…¨ä¸€è‡´ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã€Œç¬¬ã€ã‚’å«ã‚€/å«ã¾ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è©¦ã™
                if matching_rows.empty and category == "ãƒã‚¦ã‚¹":
                    if normalized_keyword.startswith("ç¬¬"):
                        # ã€Œç¬¬ã€ã‚’é™¤ã„ã¦æ¤œç´¢
                        alt_keyword = normalized_keyword[1:]
                        matching_rows = df_normalized[df_normalized[name_col] == alt_keyword]
                    else:
                        # ã€Œç¬¬ã€ã‚’è¿½åŠ ã—ã¦æ¤œç´¢
                        alt_keyword = "ç¬¬" + normalized_keyword
                        matching_rows = df_normalized[df_normalized[name_col] == alt_keyword]
                
                if not matching_rows.empty:
                    # æœ€åˆã®ä¸€è‡´è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                    matched_idx = matching_rows.index[0]
                    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰è¡Œã‚’å–å¾—
                    row = df.iloc[matched_idx]
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è©³ç´°æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§ä¿å­˜
                    detail = {
                        "ã‚«ãƒ†ã‚´ãƒª": category,
                        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keyword,
                        "CSVãƒ•ã‚¡ã‚¤ãƒ«": f"{category}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv",
                        "æ¤œç´¢åˆ—": name_col,
                        "ä¸€è‡´": "â—‹",
                        "æ­£è¦åŒ–å¾Œ": normalized_keyword
                    }
                    
                    # ä»–ã®åˆ—ã®æƒ…å ±ã‚‚è¿½åŠ 
                    for col in df.columns[1:]:
                        if pd.notna(row[col]):
                            detail[col] = row[col]
                    
                    keyword_details.append(detail)
                else:
                    # ä¸€è‡´ã—ãªã„å ´åˆã¯åŸºæœ¬æƒ…å ±ã®ã¿
                    keyword_details.append({
                        "ã‚«ãƒ†ã‚´ãƒª": category,
                        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keyword,
                        "CSVãƒ•ã‚¡ã‚¤ãƒ«": f"{category}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰.csv",
                        "æ¤œç´¢åˆ—": name_col,
                        "ä¸€è‡´": "Ã—",
                        "æ­£è¦åŒ–å¾Œ": normalized_keyword,
                        "æ³¨æ„": "è©³ç´°æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
                    })
            else:
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆ
                keyword_details.append({
                    "ã‚«ãƒ†ã‚´ãƒª": category,
                    "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keyword,
                    "æ³¨æ„": "CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ"
                })
    
    return keyword_details

# Load all prompts (ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦æ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é™¤ã)
try:
    tonmana_prompt = load_prompt("ãƒˆãƒ³ãƒãƒŠæ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ.txt")
    japanese_prompt = load_prompt("æ—¥æœ¬èªæ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ.txt")
    logic_prompt = load_prompt("ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ.txt")
    comprehensive_prompt = load_prompt("ç·åˆæ ¡æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ.txt")
except Exception as e:
    st.error(f"Error loading prompts: {e}")
    st.stop()

# Vertex AIè¨­å®šé–¢æ•°
def setup_vertex_ai(model_name, project_id=None, location=None, service_account=None):
    """Vertex AI ã‚’è¨­å®šã™ã‚‹"""
    try:
        if not VERTEX_AI_AVAILABLE:
            st.error("Vertex AI ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
            
        if not project_id:
            st.error("Project IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None, None
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼ã‚’ä½¿ç”¨
        if service_account:
            try:
                from google.oauth2 import service_account as sa
                credentials = sa.Credentials.from_service_account_info(
                    service_account,
                    scopes=['https://www.googleapis.com/auth/cloud-platform']
                )
            except Exception as e:
                st.error(f"ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                credentials = None
        else:
            credentials = None
            
        if NEW_SDK:
            if credentials:
                # èªè¨¼æƒ…å ±ã‚’ä½¿ç”¨
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = ''  # Clear any existing
                client = genai.Client(
                    vertexai=True,
                    project=project_id,
                    location=location,
                    credentials=credentials
                )
            else:
                client = genai.Client(
                    vertexai=True,
                    project=project_id,
                    location=location
                )
        else:
            genai.configure(project=project_id, location=location)
            client = None
        return client, model_name
    except Exception as e:
        st.error(f"Vertex AI ã®è¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        import traceback
        st.error(traceback.format_exc())
        return None, None

# Function to call Gemini API  
def call_gemini(prompt, user_message, model_name, project_id=None, location=None, service_account=None):
    """Gemini APIã‚’å‘¼ã³å‡ºã™"""
    try:
        client, model = setup_vertex_ai(model_name, project_id, location, service_account)
        
        if client is None and model is None:
            return None
            
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
        full_message = f"{prompt}\n\n{user_message}"
        
        if NEW_SDK:
            if client:
                response = client.models.generate_content(
                    model=model,
                    contents=full_message,
                    config=types.GenerateContentConfig(
                        max_output_tokens=1000,
                        temperature=0.1,
                    )
                )
                return response.text
            else:
                st.error("ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return None
        else:
            model_obj = genai.GenerativeModel(model)
            response = model_obj.generate_content(
                full_message,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=1000,
                    temperature=0.1,
                )
            )
            return response.text
            
    except Exception as e:
        st.error(f"Gemini APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        st.error(traceback.format_exc())
        return None

# Function to parse JSON from Claude's response
def parse_json_response(response):
    try:
        # Find JSON in the response
        start_idx = response.find('{')
        end_idx = response.rfind('}') + 1
        if start_idx >= 0 and end_idx > start_idx:
            json_str = response[start_idx:end_idx]
            return json.loads(json_str)
        else:
            st.warning("Could not find JSON in the response")
            return None
    except Exception as e:
        st.error(f"Error parsing JSON: {e}")
        st.code(response)  # Display the raw response for debugging
        return None

# Main app
st.title("mimikoæ ¡æ­£ã‚·ã‚¹ãƒ†ãƒ ")

# Initialize variables with default values
project_id_input = vertex_ai_project_id
location_input = vertex_ai_location

# Project IDãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®è­¦å‘Š
if not vertex_ai_project_id:
    st.warning("âš ï¸ Vertex AI Project IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

# Settings section
with st.expander("âš™ï¸ è¨­å®š", expanded=not vertex_ai_project_id):
    col1, col2 = st.columns(2)
    
    with col1:
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        selected_model = st.selectbox(
            "ğŸ¯ ãƒ¢ãƒ‡ãƒ«",
            vertex_model_options,
            index=0 if default_model not in vertex_model_options else vertex_model_options.index(default_model),
            key="selected_model"
        )
    
    with col2:
        project_id_input = st.text_input(
            "ğŸ“ Project ID",
            value=vertex_ai_project_id,
            help="Google Cloud ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
        location_input = st.text_input(
            "ğŸŒ Location",
            value=vertex_ai_location,
            help="Vertex AI ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
        )
    
    # æ ¡æ­£ON/OFFè¨­å®š
    st.write("### ğŸ“‹ æ ¡æ­£è¨­å®š")
    col3, col4, col5 = st.columns(3)
    
    with col3:
        enable_tonmana = st.checkbox("ğŸ¨ ãƒˆãƒ³ãƒãƒŠæ ¡æ­£", value=True, key="enable_tonmana")
    
    with col4:
        enable_japanese = st.checkbox("ğŸ“ æ—¥æœ¬èªæ ¡æ­£", value=False, key="enable_japanese")
    
    with col5:
        enable_logic = st.checkbox("ğŸ” ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£", value=True, key="enable_logic")

# Input section
st.header("å…¥åŠ›")

st.info("ç”Ÿæˆã‚¢ãƒ—ãƒªã§å‡ºåŠ›ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=['csv'])

if uploaded_file is not None:
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
    file_key = f"file_{uploaded_file.name}_{uploaded_file.size}"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰ã‚ã£ãŸã‹ãƒã‚§ãƒƒã‚¯
    if 'current_file_key' not in st.session_state or st.session_state.current_file_key != file_key:
        st.session_state.current_file_key = file_key
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€é–¢é€£ã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
        for key in list(st.session_state.keys()):
            if key.startswith('correction_') or key.startswith('selected_') or key == 'csv_data':
                del st.session_state[key]
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        df = pd.read_csv(uploaded_file, encoding='utf-8-sig')
        
        # å¿…è¦ãªåˆ—ã®å­˜åœ¨ç¢ºèª
        required_columns = ["id", "è³ªå•", "å›ç­”"]
        if not all(col in df.columns for col in required_columns):
            st.error(f"å¿…é ˆåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {required_columns}")
        else:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—ã®æ¤œå‡ºï¼ˆå‹•çš„ã«å¯¾å¿œï¼‰
            keyword_columns = []
            for col in df.columns:
                # ã‚«ãƒ†ã‚´ãƒªåã§çµ‚ã‚ã‚‹åˆ—ã‚’æ¤œå‡ºï¼ˆä¾‹: ãƒã‚¦ã‚¹1, ã‚µã‚¤ãƒ³2, ãªã©ï¼‰
                if any(col.endswith(str(i)) for i in range(1, 5)):
                    for cat in ["ãƒã‚¦ã‚¹", "ã‚µã‚¤ãƒ³", "å¤©ä½“", "ã‚¨ãƒ¬ãƒ¡ãƒ³ãƒˆ", "MPè»¸", "ã‚¿ãƒ­ãƒƒãƒˆ"]:
                        if col.startswith(cat):
                            keyword_columns.append(col)
                            break
            
            st.success(f"âœ… {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
            st.write(f"æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—: {keyword_columns}")
            
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
            with st.expander("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                st.dataframe(df.head())
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
            st.session_state.csv_data = df
            st.session_state.keyword_columns = keyword_columns
            
    except Exception as e:
        st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# CSVå‡¦ç†
if 'csv_data' in st.session_state:
    df = st.session_state.csv_data
    keyword_columns = st.session_state.keyword_columns
    
    # è¡Œé¸æŠ
    st.subheader("ğŸ“ ãƒ‡ãƒ¼ã‚¿é¸æŠ")
    
    # ãƒ‡ãƒ¼ã‚¿ä¸€è¦§ã‚’è¡¨ç¤º
    with st.expander("ãƒ‡ãƒ¼ã‚¿ä¸€è¦§", expanded=False):
        for idx, row in df.iterrows():
            st.write(f"**[{idx+1}]** ID: {row['id']} - {row['è³ªå•'][:80]}...")
    
    # æ•°å€¤å…¥åŠ›ã§é¸æŠ
    col1, col2 = st.columns([1, 3])
    with col1:
        # number_inputã‚’ä½¿ç”¨
        row_number = st.number_input(
            "ãƒ‡ãƒ¼ã‚¿ç•ªå·",
            min_value=1,
            max_value=len(df),
            value=1,
            step=1,
            help=f"1ã‹ã‚‰{len(df)}ã®ç•ªå·ã‚’å…¥åŠ›"
        )
        selected_row_idx = row_number - 1  # 0ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
    
    with col2:
        # é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ç°¡æ˜“è¡¨ç¤º
        if 0 <= selected_row_idx < len(df):
            row = df.iloc[selected_row_idx]
            st.write(f"**é¸æŠä¸­:** ID: {row['id']} - {row['è³ªå•'][:50]}...")
    
    # é¸æŠã•ã‚ŒãŸè¡Œã®ãƒ‡ãƒ¼ã‚¿
    selected_row = df.iloc[selected_row_idx]
    
    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
    with st.expander("é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®è©³ç´°", expanded=True):
        st.write(f"**ID:** {selected_row['id']}")
        st.write(f"**è³ªå•:** {selected_row['è³ªå•']}")
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¡¨ç¤º
        keywords_display = []
        for col in keyword_columns:
            if pd.notna(selected_row[col]):
                category = ''.join([c for c in col if not c.isdigit()])
                keywords_display.append(f"{category}: {selected_row[col]}")
        if keywords_display:
            st.write(f"**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:** {', '.join(keywords_display)}")
        
        st.write(f"**å›ç­”:**")
        st.text_area("", value=selected_row['å›ç­”'], height=150, disabled=True)
    
    # å€‹åˆ¥æ ¡æ­£å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("ğŸ” ã“ã®å›ç­”ã‚’æ ¡æ­£ã™ã‚‹") or f'correction_done_{selected_row_idx}' in st.session_state:
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
        if f'corrections_{selected_row_idx}' not in st.session_state:
            st.session_state[f'corrections_{selected_row_idx}'] = {}
        
        # è¨­å®šã®æº–å‚™
        current_project_id = project_id_input
        current_location = location_input
        current_service_account = gcp_service_account
        
        # è³ªå•ã¨å›ç­”ã‚’å–å¾—
        current_question = selected_row['è³ªå•']
        current_answer = selected_row['å›ç­”']
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ•´ç†
        keywords = []
        for col in keyword_columns:
            if pd.notna(selected_row[col]):
                category = ''.join([c for c in col if not c.isdigit()])
                keywords.append(f"{category}: {selected_row[col]}")
        
        # 1. ãƒˆãƒ³ãƒãƒŠæ ¡æ­£
        if f'tonmana_result_{selected_row_idx}' not in st.session_state:
            if st.session_state.get('enable_tonmana', True):
                with st.spinner("ãƒˆãƒ³ãƒãƒŠæ ¡æ­£ä¸­..."):
                    tonmana_message = f"""##QUESTION##
{current_question}

##KEYWORDS##
{', '.join(keywords) if keywords else 'ãªã—'}

##ANSWER_CAND##
{current_answer}
"""
                    tonmana_result = call_gemini(
                        tonmana_prompt, 
                        tonmana_message,
                        selected_model,
                        current_project_id,
                        current_location,
                        current_service_account
                    )
                    
                    if tonmana_result:
                        tonmana_json = parse_json_response(tonmana_result)
                        if tonmana_json:
                            st.session_state[f'tonmana_result_{selected_row_idx}'] = tonmana_result
                            st.session_state[f'tonmana_json_{selected_row_idx}'] = tonmana_json
                            
                            st.session_state[f'corrections_{selected_row_idx}']['tonmana'] = {
                                'score': tonmana_json.get('score', 0),
                                'improvements': tonmana_json.get('improvements', [])
                            }
                        else:
                            # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã€ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
                            st.error("ãƒˆãƒ³ãƒãƒŠæ ¡æ­£ã®JSONè§£æã«å¤±æ•—ã—ã¾ã—ãŸ")
                            with st.expander("ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=True):
                                st.code(tonmana_result)
            else:
                # ãƒˆãƒ³ãƒãƒŠæ ¡æ­£ãŒOFFã®å ´åˆ
                st.session_state[f'tonmana_json_{selected_row_idx}'] = {'score': 5, 'improvements': []}
                st.session_state[f'corrections_{selected_row_idx}']['tonmana'] = {
                    'score': 5,
                    'improvements': []
                }
        
        # 2. æ—¥æœ¬èªæ ¡æ­£
        if f'japanese_result_{selected_row_idx}' not in st.session_state:
            if st.session_state.get('enable_japanese', False):
                with st.spinner("æ—¥æœ¬èªæ ¡æ­£ä¸­..."):
                    japanese_result = call_gemini(
                        japanese_prompt,
                        current_answer,
                        selected_model,
                        current_project_id,
                        current_location,
                        current_service_account
                    )
                    
                    if japanese_result:
                        japanese_json = parse_json_response(japanese_result)
                        if japanese_json:
                            st.session_state[f'japanese_result_{selected_row_idx}'] = japanese_result
                            st.session_state[f'japanese_json_{selected_row_idx}'] = japanese_json
                            
                            st.session_state[f'corrections_{selected_row_idx}']['japanese'] = {
                                'score': japanese_json.get('score', 0),
                                'improvements': japanese_json.get('improvements', [])
                            }
            else:
                # æ—¥æœ¬èªæ ¡æ­£ãŒOFFã®å ´åˆ
                st.session_state[f'japanese_json_{selected_row_idx}'] = {'score': 5, 'improvements': []}
                st.session_state[f'corrections_{selected_row_idx}']['japanese'] = {
                    'score': 5,
                    'improvements': []
                }
        
        # 3. ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£
        if f'logic_result_{selected_row_idx}' not in st.session_state:
            if st.session_state.get('enable_logic', True):
                with st.spinner("ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£ä¸­..."):
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
                    keyword_details = get_keyword_details(keywords)
                    
                    # å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
                    original_keywords = []
                    arrange_keywords = []
                    
                    # CSVã®åˆ—ã‹ã‚‰å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™
                    for col in df.columns:
                        if 'å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in col and pd.notna(selected_row[col]):
                            original_keywords.append(selected_row[col])
                        elif 'ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in col and pd.notna(selected_row[col]):
                            arrange_keywords.append(selected_row[col])
                    
                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è©³ç´°æƒ…å ±ã‚’JSONå½¢å¼ã§æ•´å½¢
                    keyword_info = json.dumps(keyword_details, ensure_ascii=False, indent=2)
                    
                    # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±ã‚’ä¿å­˜
                    st.session_state[f'keyword_details_{selected_row_idx}'] = keyword_details
                    st.session_state[f'keyword_info_{selected_row_idx}'] = keyword_info
                    st.session_state[f'original_keywords_{selected_row_idx}'] = original_keywords
                    st.session_state[f'arrange_keywords_{selected_row_idx}'] = arrange_keywords
                    
                    logic_message = f"""è³ªå•: {current_question}

ä½¿ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
{keyword_info}

å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(original_keywords) if original_keywords else 'ãªã—'}

ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(arrange_keywords) if arrange_keywords else 'ãªã—'}

å›ç­”: {current_answer}
"""
                    logic_result = call_gemini(
                        logic_prompt,
                        logic_message,
                        selected_model,
                        current_project_id,
                        current_location,
                        current_service_account
                    )
                    
                    if logic_result:
                        logic_json = parse_json_response(logic_result)
                        if logic_json:
                            st.session_state[f'logic_result_{selected_row_idx}'] = logic_result
                            st.session_state[f'logic_json_{selected_row_idx}'] = logic_json
                            
                            st.session_state[f'corrections_{selected_row_idx}']['logic'] = {
                                'score': logic_json.get('score', 0),
                                'improvements': logic_json.get('improvements', [])
                            }
            else:
                # ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£ãŒOFFã®å ´åˆ
                st.session_state[f'logic_json_{selected_row_idx}'] = {'score': 5, 'improvements': []}
                st.session_state[f'corrections_{selected_row_idx}']['logic'] = {
                    'score': 5,
                    'improvements': []
                }
        
        # æ ¡æ­£å®Œäº†ãƒ•ãƒ©ã‚°
        st.session_state[f'correction_done_{selected_row_idx}'] = True
        
        # çµæœè¡¨ç¤º
        st.header("ğŸ“Š æ ¡æ­£çµæœ")
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        total_score = 0
        if f'corrections_{selected_row_idx}' in st.session_state:
            corrections = st.session_state[f'corrections_{selected_row_idx}']
            for correction_type in ['tonmana', 'japanese', 'logic']:
                if correction_type in corrections:
                    total_score += corrections[correction_type].get('score', 0)
        
        st.success(f"**ç·åˆã‚¹ã‚³ã‚¢: {total_score}/15ç‚¹**")
        
        # æ”¹å–„ç‚¹é¸æŠç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ–
        if f'selected_tonmana_{selected_row_idx}' not in st.session_state:
            st.session_state[f'selected_tonmana_{selected_row_idx}'] = []
        if f'selected_japanese_{selected_row_idx}' not in st.session_state:
            st.session_state[f'selected_japanese_{selected_row_idx}'] = []
        if f'selected_logic_{selected_row_idx}' not in st.session_state:
            st.session_state[f'selected_logic_{selected_row_idx}'] = []
        
        # 1. ãƒˆãƒ³ãƒãƒŠæ ¡æ­£çµæœ
        if f'tonmana_json_{selected_row_idx}' in st.session_state:
            st.subheader("ğŸ¨ ãƒˆãƒ³ãƒãƒŠæ ¡æ­£çµæœ")
            tonmana_json = st.session_state[f'tonmana_json_{selected_row_idx}']
            col1, col2 = st.columns([1, 3])
            with col1:
                st.metric("ã‚¹ã‚³ã‚¢", f"{tonmana_json.get('score', 0)}/5")
            
            # æ ¡æ­£ãŒOFFã®å ´åˆã®è¡¨ç¤º
            if not st.session_state.get('enable_tonmana', True):
                with col2:
                    st.info("ãƒˆãƒ³ãƒãƒŠæ ¡æ­£ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")
            
            improvements = tonmana_json.get('improvements', [])
            if improvements:
                st.write("**æ”¹å–„ç‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„:**")
                for i, improvement in enumerate(improvements):
                    is_selected = improvement in st.session_state[f'selected_tonmana_{selected_row_idx}']
                    if st.checkbox(improvement, key=f"tonmana_cb_{selected_row_idx}_{i}", value=is_selected):
                        if improvement not in st.session_state[f'selected_tonmana_{selected_row_idx}']:
                            st.session_state[f'selected_tonmana_{selected_row_idx}'].append(improvement)
                    else:
                        if improvement in st.session_state[f'selected_tonmana_{selected_row_idx}']:
                            st.session_state[f'selected_tonmana_{selected_row_idx}'].remove(improvement)
            else:
                st.info("æ”¹å–„ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        # 2. æ—¥æœ¬èªæ ¡æ­£çµæœ
        if f'japanese_json_{selected_row_idx}' in st.session_state:
            st.subheader("ğŸ“ æ—¥æœ¬èªæ ¡æ­£çµæœ")
            japanese_json = st.session_state[f'japanese_json_{selected_row_idx}']
            col1, col2 = st.columns([1, 3])
            with col1:
                st.metric("ã‚¹ã‚³ã‚¢", f"{japanese_json.get('score', 0)}/5")
            
            # æ ¡æ­£ãŒOFFã®å ´åˆã®è¡¨ç¤º
            if not st.session_state.get('enable_japanese', False):
                with col2:
                    st.info("æ—¥æœ¬èªæ ¡æ­£ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")
            
            improvements = japanese_json.get('improvements', [])
            if improvements:
                st.write("**æ”¹å–„ç‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„:**")
                for i, improvement in enumerate(improvements):
                    is_selected = improvement in st.session_state[f'selected_japanese_{selected_row_idx}']
                    if st.checkbox(improvement, key=f"japanese_cb_{selected_row_idx}_{i}", value=is_selected):
                        if improvement not in st.session_state[f'selected_japanese_{selected_row_idx}']:
                            st.session_state[f'selected_japanese_{selected_row_idx}'].append(improvement)
                    else:
                        if improvement in st.session_state[f'selected_japanese_{selected_row_idx}']:
                            st.session_state[f'selected_japanese_{selected_row_idx}'].remove(improvement)
            else:
                st.info("æ”¹å–„ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        # 3. ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£çµæœ
        if f'logic_json_{selected_row_idx}' in st.session_state:
            st.subheader("ğŸ” ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£çµæœ")
            logic_json = st.session_state[f'logic_json_{selected_row_idx}']
            col1, col2 = st.columns([1, 3])
            with col1:
                st.metric("ã‚¹ã‚³ã‚¢", f"{logic_json.get('score', 0)}/5")
            
            # æ ¡æ­£ãŒOFFã®å ´åˆã®è¡¨ç¤º
            if not st.session_state.get('enable_logic', True):
                with col2:
                    st.info("ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è©³ç´°ã‚’è¡¨ç¤º
            with st.expander("ğŸ”§ ãƒ‡ãƒãƒƒã‚°: AIã«é€ä¿¡ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æƒ…å ±", expanded=False):
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
                st.write("**ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆCSVã‹ã‚‰å–å¾—ï¼‰:**")
                st.write(f"{', '.join(keywords) if keywords else 'ãªã—'}")
                
                # å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                if f'original_keywords_{selected_row_idx}' in st.session_state:
                    st.write("\n**å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:**")
                    orig_kws = st.session_state[f'original_keywords_{selected_row_idx}']
                    st.write(f"{', '.join(orig_kws) if orig_kws else 'ãªã—'}")
                
                if f'arrange_keywords_{selected_row_idx}' in st.session_state:
                    st.write("\n**ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:**")
                    arr_kws = st.session_state[f'arrange_keywords_{selected_row_idx}']
                    st.write(f"{', '.join(arr_kws) if arr_kws else 'ãªã—'}")
                
                if f'keyword_details_{selected_row_idx}' in st.session_state:
                    keyword_details = st.session_state[f'keyword_details_{selected_row_idx}']
                    st.write("\n**ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è©³ç´°æƒ…å ±ï¼ˆå„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å†…å®¹ï¼‰:**")
                    
                    for i, detail in enumerate(keyword_details, 1):
                        st.write(f"\n**[{i}] {detail.get('ã‚«ãƒ†ã‚´ãƒª', '')}: {detail.get('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰', '')}**")
                        
                        # è©³ç´°æƒ…å ±ã‚’è¡¨å½¢å¼ã§è¡¨ç¤º
                        detail_items = []
                        for key, value in detail.items():
                            if key not in ['ã‚«ãƒ†ã‚´ãƒª', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰']:
                                detail_items.append(f"- **{key}**: {value}")
                        
                        if detail_items:
                            for item in detail_items:
                                st.write(item)
                        else:
                            st.write("- è©³ç´°æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    
                    st.write("\n**JSONå½¢å¼ï¼ˆAIã«é€ä¿¡ã•ã‚ŒãŸå†…å®¹ï¼‰:**")
                    st.code(st.session_state[f'keyword_info_{selected_row_idx}'], language='json')
            
            improvements = logic_json.get('improvements', [])
            if improvements:
                st.write("**æ”¹å–„ç‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„:**")
                for i, improvement in enumerate(improvements):
                    is_selected = improvement in st.session_state[f'selected_logic_{selected_row_idx}']
                    if st.checkbox(improvement, key=f"logic_cb_{selected_row_idx}_{i}", value=is_selected):
                        if improvement not in st.session_state[f'selected_logic_{selected_row_idx}']:
                            st.session_state[f'selected_logic_{selected_row_idx}'].append(improvement)
                    else:
                        if improvement in st.session_state[f'selected_logic_{selected_row_idx}']:
                            st.session_state[f'selected_logic_{selected_row_idx}'].remove(improvement)
            else:
                st.info("æ”¹å–„ç‚¹ã¯ã‚ã‚Šã¾ã›ã‚“")
        
        # ç·åˆæ ¡æ­£ãƒœã‚¿ãƒ³
        st.header("âœ¨ ç·åˆæ ¡æ­£")
        if st.button("é¸æŠã—ãŸæ”¹å–„ç‚¹ã§ç·åˆæ ¡æ­£ã‚’å®Ÿè¡Œ") or f'comprehensive_result_{selected_row_idx}' in st.session_state:
            if f'comprehensive_result_{selected_row_idx}' not in st.session_state:
                with st.spinner("ç·åˆæ ¡æ­£ä¸­..."):
                    # é¸æŠã•ã‚ŒãŸæ”¹å–„ç‚¹ã‚’æ•´ç†
                    selected_improvements = {
                        "ãƒˆãƒ³ãƒãƒŠæ ¡æ­£": st.session_state.get(f'selected_tonmana_{selected_row_idx}', []),
                        "æ—¥æœ¬èªæ ¡æ­£": st.session_state.get(f'selected_japanese_{selected_row_idx}', []),
                        "ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£": st.session_state.get(f'selected_logic_{selected_row_idx}', [])
                    }
                    
                    # ã‚¹ã‚³ã‚¢ã‚’æ•´ç†
                    scores = {}
                    if f'corrections_{selected_row_idx}' in st.session_state:
                        corrections = st.session_state[f'corrections_{selected_row_idx}']
                        if 'tonmana' in corrections:
                            scores["ãƒˆãƒ³ãƒãƒŠæ ¡æ­£"] = corrections['tonmana'].get('score', 0)
                        if 'japanese' in corrections:
                            scores["æ—¥æœ¬èªæ ¡æ­£"] = corrections['japanese'].get('score', 0)
                        if 'logic' in corrections:
                            scores["ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£"] = corrections['logic'].get('score', 0)
                    
                    # ç·åˆæ ¡æ­£ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
                    comprehensive_message = f"""AIå ã„å¸«ã®å›ç­”:
{current_answer}

ä½¿ç”¨ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
{', '.join(keywords) if keywords else 'ãªã—'}

å„æ ¡æ­£AIã®æ¡ç‚¹çµæœ:
"""
                    for correction_type, score in scores.items():
                        comprehensive_message += f"{correction_type}: {score}/5\n"
                    
                    comprehensive_message += "\né¸æŠã•ã‚ŒãŸæ”¹å–„ç‚¹:\n"
                    for correction_type, improvements in selected_improvements.items():
                        if improvements:
                            comprehensive_message += f"\n{correction_type}:\n"
                            for i, improvement in enumerate(improvements):
                                comprehensive_message += f"{i+1}. {improvement}\n"
                    
                    # ç·åˆæ ¡æ­£å®Ÿè¡Œ
                    comprehensive_result = call_gemini(
                        comprehensive_prompt,
                        comprehensive_message,
                        selected_model,
                        current_project_id,
                        current_location,
                        current_service_account
                    )
                    
                    if comprehensive_result:
                        st.session_state[f'comprehensive_result_{selected_row_idx}'] = comprehensive_result
            
            # ç·åˆæ ¡æ­£çµæœè¡¨ç¤º
            if f'comprehensive_result_{selected_row_idx}' in st.session_state:
                st.subheader("ç·åˆæ ¡æ­£çµæœ")
                st.write(st.session_state[f'comprehensive_result_{selected_row_idx}'])
    
    # åŒºåˆ‡ã‚Šç·š
    st.divider()
    
    # ä¸€æ‹¬å‡¦ç†ãƒœã‚¿ãƒ³
    st.header("ğŸš€ ä¸€æ‹¬å‡¦ç†")
    if st.button("å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬æ ¡æ­£"):
        # çµæœã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã®åˆ—ã‚’è¿½åŠ 
        df['ãƒˆãƒ³ãƒãƒŠã‚¹ã‚³ã‚¢'] = 0
        df['æ—¥æœ¬èªã‚¹ã‚³ã‚¢'] = 0
        df['ãƒ­ã‚¸ãƒƒã‚¯ã‚¹ã‚³ã‚¢'] = 0
        df['ç·åˆã‚¹ã‚³ã‚¢'] = 0
        df['æ”¹å–„ç‚¹'] = ""
        df['ç·åˆæ ¡æ­£çµæœ'] = ""
        
        # è¨­å®šã®æº–å‚™
        current_project_id = project_id_input
        current_location = location_input
        current_service_account = gcp_service_account
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # å„è¡Œã‚’å‡¦ç†
        for index, row in df.iterrows():
            status_text.text(f"å‡¦ç†ä¸­: {index + 1}/{len(df)}")
            
            # è³ªå•ã¨å›ç­”ã‚’å–å¾—
            current_question = row['è³ªå•']
            current_answer = row['å›ç­”']
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ•´ç†
            keywords = []
            for col in keyword_columns:
                if pd.notna(row[col]):
                    # ã‚«ãƒ†ã‚´ãƒªåã‚’æŠ½å‡ºï¼ˆä¾‹: "ãƒã‚¦ã‚¹1" -> "ãƒã‚¦ã‚¹"ï¼‰
                    category = ''.join([c for c in col if not c.isdigit()])
                    keywords.append(f"{category}: {row[col]}")
            
            # 1. ãƒˆãƒ³ãƒãƒŠæ ¡æ­£
            if st.session_state.get('enable_tonmana', True):
                tonmana_message = f"""##QUESTION##
{current_question}

##KEYWORDS##
{', '.join(keywords) if keywords else 'ãªã—'}

##ANSWER_CAND##
{current_answer}
"""
                tonmana_result = call_gemini(
                    tonmana_prompt, 
                    tonmana_message,
                    selected_model,
                    current_project_id,
                    current_location,
                    current_service_account
                )
                
                tonmana_json = parse_json_response(tonmana_result) if tonmana_result else None
                if tonmana_json:
                    df.at[index, 'ãƒˆãƒ³ãƒãƒŠã‚¹ã‚³ã‚¢'] = tonmana_json.get('score', 0)
                    improvements = tonmana_json.get('improvements', [])
                    if improvements:
                        df.at[index, 'æ”¹å–„ç‚¹'] += f"ã€ãƒˆãƒ³ãƒãƒŠã€‘{', '.join(improvements)}\n"
            else:
                df.at[index, 'ãƒˆãƒ³ãƒãƒŠã‚¹ã‚³ã‚¢'] = 5
            
            # 2. æ—¥æœ¬èªæ ¡æ­£
            if st.session_state.get('enable_japanese', False):
                japanese_result = call_gemini(
                    japanese_prompt,
                    current_answer,
                    selected_model,
                    current_project_id,
                    current_location,
                    current_service_account
                )
                
                japanese_json = parse_json_response(japanese_result) if japanese_result else None
                if japanese_json:
                    df.at[index, 'æ—¥æœ¬èªã‚¹ã‚³ã‚¢'] = japanese_json.get('score', 0)
                    improvements = japanese_json.get('improvements', [])
                    if improvements:
                        df.at[index, 'æ”¹å–„ç‚¹'] += f"ã€æ—¥æœ¬èªã€‘{', '.join(improvements)}\n"
            else:
                df.at[index, 'æ—¥æœ¬èªã‚¹ã‚³ã‚¢'] = 5
            
            # 3. ãƒ­ã‚¸ãƒƒã‚¯æ ¡æ­£
            if st.session_state.get('enable_logic', True):
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
                keyword_details = get_keyword_details(keywords)
                
                # å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—
                original_keywords = []
                arrange_keywords = []
                
                # CSVã®åˆ—ã‹ã‚‰å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã™
                for col in df.columns:
                    if 'å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in col and pd.notna(row[col]):
                        original_keywords.append(row[col])
                    elif 'ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰' in col and pd.notna(row[col]):
                        arrange_keywords.append(row[col])
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è©³ç´°æƒ…å ±ã‚’JSONå½¢å¼ã§æ•´å½¢
                keyword_info = json.dumps(keyword_details, ensure_ascii=False, indent=2)
                
                logic_message = f"""è³ªå•: {current_question}

ä½¿ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
{keyword_info}

å…ƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(original_keywords) if original_keywords else 'ãªã—'}

ã‚¢ãƒ¬ãƒ³ã‚¸ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(arrange_keywords) if arrange_keywords else 'ãªã—'}

å›ç­”: {current_answer}
"""
                logic_result = call_gemini(
                    logic_prompt,
                    logic_message,
                    selected_model,
                    current_project_id,
                    current_location,
                    current_service_account
                )
                
                logic_json = parse_json_response(logic_result) if logic_result else None
                if logic_json:
                    df.at[index, 'ãƒ­ã‚¸ãƒƒã‚¯ã‚¹ã‚³ã‚¢'] = logic_json.get('score', 0)
                    improvements = logic_json.get('improvements', [])
                    if improvements:
                        df.at[index, 'æ”¹å–„ç‚¹'] += f"ã€ãƒ­ã‚¸ãƒƒã‚¯ã€‘{', '.join(improvements)}\n"
            else:
                df.at[index, 'ãƒ­ã‚¸ãƒƒã‚¯ã‚¹ã‚³ã‚¢'] = 5
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            total_score = df.at[index, 'ãƒˆãƒ³ãƒãƒŠã‚¹ã‚³ã‚¢'] + df.at[index, 'æ—¥æœ¬èªã‚¹ã‚³ã‚¢'] + df.at[index, 'ãƒ­ã‚¸ãƒƒã‚¯ã‚¹ã‚³ã‚¢']
            df.at[index, 'ç·åˆã‚¹ã‚³ã‚¢'] = total_score
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°
            progress_bar.progress((index + 1) / len(df))
        
        status_text.text("å‡¦ç†å®Œäº†!")
        
        # çµæœè¡¨ç¤º
        st.subheader("æ ¡æ­£çµæœã‚µãƒãƒªãƒ¼")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            avg_tonmana = df['ãƒˆãƒ³ãƒãƒŠã‚¹ã‚³ã‚¢'].mean()
            st.metric("å¹³å‡ãƒˆãƒ³ãƒãƒŠã‚¹ã‚³ã‚¢", f"{avg_tonmana:.2f}/5")
        
        with col2:
            avg_japanese = df['æ—¥æœ¬èªã‚¹ã‚³ã‚¢'].mean()
            st.metric("å¹³å‡æ—¥æœ¬èªã‚¹ã‚³ã‚¢", f"{avg_japanese:.2f}/5")
        
        with col3:
            avg_logic = df['ãƒ­ã‚¸ãƒƒã‚¯ã‚¹ã‚³ã‚¢'].mean()
            st.metric("å¹³å‡ãƒ­ã‚¸ãƒƒã‚¯ã‚¹ã‚³ã‚¢", f"{avg_logic:.2f}/5")
        
        with col4:
            avg_total = df['ç·åˆã‚¹ã‚³ã‚¢'].mean()
            st.metric("å¹³å‡ç·åˆã‚¹ã‚³ã‚¢", f"{avg_total:.2f}/15")
        
        # çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        with st.expander("çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=True):
            st.dataframe(df[['id', 'è³ªå•', 'ãƒˆãƒ³ãƒãƒŠã‚¹ã‚³ã‚¢', 'æ—¥æœ¬èªã‚¹ã‚³ã‚¢', 'ãƒ­ã‚¸ãƒƒã‚¯ã‚¹ã‚³ã‚¢', 'ç·åˆã‚¹ã‚³ã‚¢', 'æ”¹å–„ç‚¹']].head(10))
        
        # CSVå‡ºåŠ›
        output_buffer = io.StringIO()
        df.to_csv(output_buffer, index=False, encoding='utf-8-sig')
        
        st.download_button(
            label="ğŸ“¥ æ ¡æ­£çµæœã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=output_buffer.getvalue(),
            file_name=f"mimiko_correction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )